%!TEX root = thesis.tex
% Start of content
\section{Introduction}
\label{sec:intro}

Currently, as a society, we are generating very large amounts of data from a large range of different sources. These
sources include scientific experiments, such as the Australian Synchrotron~\cite{web:synchrotron} and The Large Hadron
Collider~\cite{web:LHC}, companies, such as Amazon~\cite{web:Amazon}, and also data generated by end users of products,
such as social networks. The rate of data that is being generated is constantly increasing, presenting major challenges
when it comes to the storage and processing of that data~\cite{bohlouli_towards_2013}. This is what is often referred to
now as ``Big Data''. A further big data project, that will be looked into as the basis of the project outlined in this
thesis, is the automated monitoring of railway tracks and cars by the Institute of Railway Technology at Monash University
(IRT)~\cite{web:monash_irt}.

Out of all of these data that are faced in such projects, often only specific parts of the data are of particular use for given
purposes. Hence, rather than attempting to store all the new data that is being generated, an increasingly popular method of
dealing with such data, in both academia and industry associated with big data, is the processing and analysis of
data in realtime as it is received.

There are currently numerous realtime data processing frameworks that are in development and in production use, both in
industry and academia. Examples of these realtime data processing frameworks include the widely used Storm
project~\cite{web:Storm}, developed at BackType and Twitter, Inc., and also the up-and-coming Spark Streaming
project~\cite{web:SparkStreaming}, developed at UC Berkeley's AMPLab~\cite{web:UCBerkelyAMCLab}, both of which are
open-source projects. While there are a growing number of these projects being developed, often these projects are designed
with a particular type of data in mind, or to facilitate a particular type of data processing. For example, the before
mentioned Spark Streaming project, along with its mother project, Spark~\cite{web:Spark}, was originally designed for highly
parallelisable data with the use-case in mind of processing data in-memory using highly iterative machine learning
algorithms related to data analytics~\cite{liu_survey_2014}.

What is proposed in this chapter is a realtime big data processing system for the aforementioned railway monitoring system by the IRT at Monash University given the possibility that data is able to be streamed in realtime from the railway in realtime.

This chapter will be structured as follows:\\
A brief outline of the domain on big data processing, both batch and realtime processing, will be given
in~\sectref{sec:research_context}. Furthermore, an overview of the Monash University Institute of Railway Technology's
project, upon which this project is based on, will be covered in the aforementioned chapter. In~\sectref{sec:objectives},
the goals and aims of the project will be given, as well as more specific goals and aims relating to the subproject
covered in this thesis. Research questions for this subproject will be given in~\sectref{sub:research_questions}.
Finally, the structure of this overall thesis will be given in~\sectref{sub:proposed_thesis_chapter_headings}, before
concluding this chapter in~\sectref{sec:summary}.

% section introduction (end)



\subsection{Research Context} % (fold)
\label{sec:research_context}

\subsubsection{Big Data} % (fold)
\label{sub:big_data}

Big data, as explained previously, is becoming commonplace in both industry and academia. Everyday companies are finding
that they are generating too much data and that their traditional relational database management system (RDMBS) solutions cannot
scale to the epic proportions needed to handle this data in an efficient and robust manner~\cite{marz2013principles}.
Hence, companies and academics alike have started looking at alternative solutions designed with the goal of handling
these massive datasets.

The most popular solution for this problem, up until recently, has been the MapReduce model of programming along with
some type of scalable distributed storage system~\cite{bifet_mining_2013}. The MapReduce model was started at Google,
Inc.\@ with their own proprietary implementation along with their proprietary distributed file system, known as the Google
File System (GFS)~\cite{ghemawat_google_2003}. Without going into the low-level details of MapReduce and GFS, the use of this solution at Google
allowed the company to easily handle all the data that was coming into their servers, including that related to Google Search, and perform the necessary
processing operations that was needed at the time~\cite{ghemawat_google_2003}~\cite{dean_mapreduce:_2008}.

% subsection big_data (end)

\subsubsection{Batch data processing} % (fold)
\label{sub:apache hadoop}

From the success of MapReduce usage combined with GFS at Google, the open-source community responded swiftly with the
development of the Apache Hadoop framework. Hadoop originally offered an open-source implementation of MapReduce and
their own open-source distributed file system known as the Hadoop Distributed File System
(HDFS)~\cite{shvachko_hadoop_2010}.

Hadoop soon became the subject of mass-adoption in both industry and academia, being deployed at a fast rate.
Development of the Hadoop framework also grew at a fast rate, with new applications related to HDFS and MapReduce being
built on top of Hadoop, greatly benefiting the ecosystem as a whole. Some of these applications grew into widely adopted
systems in their own right. For example, Hadoop applications such as Apache Pig~\cite{gates_building_2009} and
Hive~\cite{thusoo_hive_2010} allow for easy querying and manipulation of data stored on HDFS, both coming with the
addition of their own query languages~\cite{olston_pig_2008}.

Additionally, as further non-MapReduce model applications became of interest to the Hadoop community, Hadoop soon
developed a further abstraction on top of the underlying resources (in most cases, HDFS). The goal of this was to
facilitate the development and deployment of many different applications, varying in use-case, which could be run on the
Hadoop ecosystem, without forcing developers to fit their application into the MapReduce model. This development was
known as Apache Hadoop YARN:\@ Yet Another Resource Negotiator, which can be thought of as an operating system-like abstraction sitting
atop of the available Hadoop resources~\cite{vavilapalli_apache_2013}. The abstraction YARN provides facilitated the
development of much more advanced, and non-MapReduce technologies which have since become widely used parts of the
Hadoop ecosystem~\cite{harrison_hadoops_2012}.

% subsection apache_hadoop (end)

\subsubsection{Realtime data processing} % (fold)
\label{sub:prop_realtime_data_processing}

One of the major limitations of Hadoop, and the MapReduce model in general, soon became obvious:\@ MapReduce was designed
with the goal of being able to process batches of data, hence, given Hadoop's dominance, batched data processing was the
focal point of the entire distributed data processing domain~\cite{kamburugamuve_survey_2014}. Essentially, batched data
processing is where data gets collected first into large enough batches before being processed all-at-once. The point of
processing in such a way is so there would be less overheads than attempting to process each individual datum as it
arrives. For a lot of use-cases this was, and still is, fine as there were no other drawbacks apart from a high level of
latency between the stages of when the data arrives and when it gets processed. However, for other applications, such as
stock trading, sensor monitoring, and web traffic processing, a more low-latency, realtime solution was
needed~\cite{kamburugamuve_survey_2014}.

Soon, many solutions, with different use-cases and design goals, were developed in the area of distributed stream
processing systems (DSPS). Given the Hadoop ecosystem that was already widely adopted, most of these DSPSs were built
upon the still new YARN layer, ensuring overall compatibility with the Hadoop ecosystem, and the underlying HDFS. Some
examples of such projects include the beforementioned Apache Storm, currently being used at Twitter,
Inc.~\cite{toshniwal_stormtwitter_2014}, among many other companies. Also up-and-coming projects, such as Apache Samza
which is a recently open-sourced project, currently being used in production at LinkedIn Corporation~\cite{web:Samza}.

% subsection realtime_data_processing (end)

\subsubsection{Monash IRT Railway Project} % (fold)
\label{sub:monash_irt_railway_project}

The railway project that has been developed at Monash University's Institute of Railway Technology uses numerous sensor
technologies on certain train cars, such as the Track Geometry Recording Car (TGRC) and the Instrumented Ore Car (IOC),
to monitor railway track conditions and detect track abnormalities~\cite{darby2003development}~\cite{darby2005track}.
These train cars operate in the Pilbara region of Western Australia, continously performing round trips from a port
to a given loading point, where they are loaded with recently mined minerals and ores.

As is currently the case, data is received and processed using batch data processing technologies. Due to limited coverage
of cellular networks in the Pilbara region of Western Australia, in which the trains currently operate, sensor data from
a given trip is automatically transmitted in large batches to be received by remote servers once a train has concluded a
round trip and
arrives back in port from a loading point~\cite{thomas2012taking}. Given the current cellular network infrastructure in
the region, this is the only feasible option for transmission of data, however Monash IRT have indicated that given
future improvements in cellular network infrastructure, streaming the data back to remote servers in realtime is a likely
possibility. This leads to the potential possibility of this research project's outcomes outlined in this thesis.

The form of batch handling and processing performed on the railway data currently leads to a number of limitations and
problems. The data is currently stored in a relational database management system (RDBMS), and with the current
implementation
of the sensors, the data received is not consistently structured. In fact, the only sensor data guaranteed to be received
in each batch is geographic location and time. Due to the highly structured nature of RDBMS technology, certain work-arounds
need to be performed on the data so that it is compliant to RDBMS schemas, such as the insertions of default values in
in the case of missing attributes. Furthermore, low query performance has been noted as a problem plaguing the IRT team
working with the railway data. They wish to resolve these problems by looking into non-relational models for their data
storage and processing systems.

% subsection monash_irt_railway_project (end)

% section research_context (end)


\subsection{Research Objectives} % (fold)
\label{sec:objectives}

The main aim or goal of this research project is to develop a fully automated, non-relational big data system to manage
the data received from railway car sensors as a part of Monash University's Institute of Railway Technology project. The
intention is to replace their current relational solution, offering at least the same capabilities at a larger scale, and
with higher performance. The scope of this project is relatively large, and the project, or subproject, outlined in this
thesis only covers a portion of the greater aforementioned project.

The main aim of this subproject is to look at the hypothetical possibility of dealing with the railway sensor data being
streamed in realtime. This will also involve non-relational big data systems, however the details of these such systems
in the scope of the greater project will be left up to other individuals working on those subprojects.

To allow the possibility of realtime data streaming from the railway sensors, appropriate data stream processing system
(DSPS) technology needs to be looked at, tested, and evaluated. This makes up the core part of this subproject's work.
These DSPS technologies need to appropriately take, or accept, the data from some specified source, \eg the sensors, apply
any realtime processing logic that is required, \eg pre-processing, then forward the data on for handling at
another source, \eg long term storage, such as HDFS.

This DSPS system will act as a sort of processing ``pipeline'' through which the sensor data flows.
By the end of this subproject, we aim to have proof-of-concept implementations of the pipeline working on the National
eResearch Collaboration Tools and Resources (NeCTAR) cloud services~\cite{web:Nectar}, making use of certain DSPS
technologies.

% section objectives (end)



\subsection{Research Questions} % (fold)
\label{sub:research_questions}

The following research questions are the main focus points of this subproject:

\begin{enumerate}
  \item\label{item:dsps} How can existing DSPS technologies be used to fill this realtime processing gap in the Monash
  University's IRT project?
  \item\label{item:pipeline} What qualitative and quantitative tests can be performed to recommend a particular DSPS
  technology for building the pipeline?
  \item\label{item:recommendations} How can we design the DSPS processing pipeline to be extensible, allowing for the
  addition of future realtime processing requirements?
\end{enumerate}

Additionally, after answering each of these preliminary research questions, we will want to properly implement the
theoretical discoveries from each stage. We do this with the goal of achieving some deployable pipeline that can be then
be used in the testing and overall evaluation stages.

% subsection research_questions (end)


\subsection{Thesis Structure} % (fold)
\label{sub:proposed_thesis_chapter_headings}

The proposed structure of the remainder of the thesis is as follows. \sectref{sec:litrev} looks at prior
research and work into the area of big data stream processing, performed both in industry and academia.
A clear overview of the DSPS technologies chosen for this project will be given in~\sectref{sec:overview},
along with detailing the experiments that will be performed and evaluated. \sectref{sec:implementation}
will detail how the experimental systems built
for this project were implemented using the DSPS technologies highlighted in~\sectref{sec:overview},
with an evaluation of the experiments being detailed in~\sectref{sec:evaluation}.
Finally,~\sectref{sec:conclusion} will conclude the thesis, along with highlighting any further research
gaps that have been made apparent as the result of this project.

% subsection proposed_thesis_chapter_headings (end)


\subsection{Summary} % (fold)
\label{sec:summary}

This chapter has introduced the overall project, and in-turn the subproject upon which this thesis will be based. It
has described the current state of the Monash University Institute of Railway Technology's project and the current
problems that they are faced with, such as the need to deal with non-consistently structured data and low query
performance. These problems are exacerbated given the non-realtime nature of the current data, where issues with sensors
are only discovered much later after-the-fact. To overcome these issues, and look forward into the hypothetical,
but likely, possibility of having the technological infrastructure to support realtime data processing, this research
aims to develop a realtime processing pipeline, taking the data straight from the railway sensors and perform any
needed realtime processing.

This chapter also outlined a brief context of the research project and big data as a whole, the scope of this subproject,
this subproject's research questions. Finally it was concluded with an overview of this thesis' entire structure. The
following chapter will look into previous research that has been done on realtime big data processing and handling,
along with going into more detail of the railway project's needs.


% section summary (end)
