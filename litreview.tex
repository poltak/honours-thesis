
% Start of content

\section{Introduction} % (fold)
\label{sec:litrev_introduction}

The realtime processing of big data is of great importance to both academia and industry. Advancements and progress in
modern society can be directly attributed back to data. The value of data has become more apparent, and data has become
a sort of currency for the information economy~\cite{st2009examining}. Hence, those in society who realised the value of
data early hold immense power over the entire economy, and in turn society, overall~\cite{lievesley1993increasing}.
From seemingly inconsequential gains at the macro level, such as the ability to more
accurately predict the rise and fall of airline tickets~\cite{darlin2006airfares}, to those of utmost importance for
society as a whole, such as predicting and tracking the spread of the Swine Flu Pandemic in 2009 more accurately than
the United States Centers for Disease Control and Prevention could~\cite{ritterman2009using,mayer2013big}. It is
applications of big data processing like these that have been recognised by academics and organisations in
industry alike, with the last decade seeing a major shift in research and development into new methods for the handling
and processing of big data.

This review will give a background on the types and classes of big data, as well as the various methods employed to
process those given classes of data. We will more specifically be focusing on the methods that are involved with the
analysis and processing of realtime data streams, as opposed to the batch processing of big data. This review will look
into detail at previous work that has been done in the field of big data, specifically those works that have had a
greater influence on the field  as a whole. This includes both works looking specifically at the processing of streaming
data, and works involving processed big data in batch mode, given that batch mode processing arguably led onto the
current hot-topic of realtime stream processing.

This review will be structured in three main sections. In~\sectref{sec:big_data_types_background}, an overview of the
different classes and types of big data will be presented. This section will be concluded with a discussion on the
literature that has been reviewed, along with a comparison of the different classifications.
In~\sectref{sec:big_data_processing_background}, an overview will be given of the major open-source big data processing
systems in the scope of the Hadoop ecosystem. A special emphasis will be given to realtime data processing systems,
otherwise known as data stream processing systems (DSPSs), given that the main area of this research is focusing on
realtime data processing. This section will also be concluded with a brief comparison of the presented systems.
In~\sectref{sec:conclusion} an analysis and discussion will be given on the content covered from the relevant
literature, as well as identifying how the research contributions of this project will address the gaps found in the
relevant literature.

% section introduction (end)

\newpage


\section{Data types and characteristics background} % (fold)
\label{sec:big_data_types_background}

\subsection{Velocity, variety, volume, and veracity} % (fold)
\label{sub:four_v}

Data, and more specifically, big data, are often characterised into what is known as the ``four
V's''~\cite{wang2014bigdatabench}. These can be thought of as different ``dimensions'' of big data, and can be
summarised as follows~\cite{dong2013big}:

\begin{itemize}
  \item \emph{Velocity:} The rate at which data is being collected and made available to the data consumers.
  \item \emph{Variety:} The heterogeneity of data. Big data often exhibits substantial variations in both the structural
  level and the instance level (representations of real-world entities). This is often highlighted by data systems that
  depend on acquiring of data from a number of non-conforming, and sometimes unrelated, data sources.
  \item \emph{Volume:} The amount of data that is obtained by the data consumer from the data source/s.
  \item \emph{Veracity:} The quality, in terms of accuracy, coverage, and timeliness, of data that is consumed from
  the data source/s. Veracity of data can widely differ between sources.
\end{itemize}

While the four V's are often described in terms of big data, they can also apply to more traditional data warehousing
and processing in general, albeit on a far smaller scale. In the domain of big data processing, data will exhibit signs
of high velocity, variety, and volume~\cite{beyer2011gartner}, and hence the veracity of the data may also fluctuate.
Meanwhile, in more traditional data processing, the scope may be limited, especially in terms of factors such as variety
and, as a consequence, there is less need of an emphasis on veracity due to limited variety in data sources.

As will be made clear in the following sections, a lot of the identified classes and characteristics of data directly
relate back to these four V's. These can be considered the underlying features of many characteristics of data, both in
the sense of big data and traditional data.

% subsection velocity_variety_volume_and_veracity (end)


\subsection{Classification of data} % (fold)
\label{sub:data_classification}

Data, in general, can be categorised into a number of different classes or types. We define the concept of a data class
to mean the same as the terms of ``data type'', ``data category'', or ``data format'', as all terms were often used
interchangeably in other literature.

Each class of data can be further defined and categorised via the characteristics they exhibit. Furthermore, these
characteristics exhibited by data classes can be exploited and it is often possible to optimise the processing of each
class of data by processing it using a specific method depending on those characteristics.

To give an example of this, data that is expected to have highly iterative processing applied to it would benefit from a
data processor that does not have to unnecessarily write to disk after every single iteration. The elimination of this
I/O overhead is an example of the optimisations that could be applied to the overall process from correctly identifying
the data class beforehand, and processing it accordingly.

Furthermore, particular classes of data are generally only found in particular applications or use cases of data
processing. As this is the case, it narrows down the amount of classification needed, depending on the application that
is being looked at. This will be elaborated on in later parts of this section.

There is no concrete, universally accepted standard for the classification of data. While the study of big data
processing can arguably be considered still in its infancy, data handling and processing in general is relatively
mature. From preliminary research on looking at past work and literature in this area, it must be noted that there is a
significant lack of research on the classification of data.

The literature that will be reviewed in this section is often not wholly focused on the idea of data classification,
hence data classification is presented relative to whatever the overall topic of the literature is on. This is important
to note, as one attempt at data classification may not be appropriate under a different context. This also explains the
large variation in different classification attempts, although we will also highlight the recurring similarities between
different data classification literature.


\subsubsection{IBM's classification of big data}
\label{ssub:data_charact}

This section looks at a classification of big data types, along with the key data characteristics, proposed by IBM
Architects Mysore, Khupat, and Jain, published by IBM in 2013~\cite{ibm_big_2013}. The content is targeted towards
beginners in the area of big data processing; much like the set of recommendations that we intend to produce from this
research project. The different data classes, or ``formats'' as they were labelled, that are commonly encountered in big
data are identified. For each of these formats, the underlying characteristics of the data was discussed, and it was
noted that the type of processing needed would be dependent on those characteristics.

\noindent The characteristics of data, as put forward by Mysore et al., in~\cite{ibm_big_2013}, include the following:

\noindent \textbf{Analysis type -} Whether or not the data would be processed/analysed in realtime, or batched for later
processing. Often this data class characteristic is dependent on the application of the data, \eg{}the processing of
social media data for the analysis of currently occurring events would want to be processed in realtime, regardless of
the type of data that is involved.

\noindent \textbf{Processing methodology -} This characteristic involves the approach used when processing the data.
Some examples of different processing methodologies include: predictive processing, analytical, ad-hoc queries, and
reporting. Often the processing methodology for a particular class is determined by the business requirements or
application of the data. Depending on the processing methodology used, many different combinations of big data
technologies can be used.

\noindent \textbf{Data frequency and size -} The amount of data expected to arrive to the processing system, along with
the speed and regularity of the incoming data. Knowing this characteristic beforehand can determine the methods for data
storage and preprocessing, if needed. Examples of data frequency includes: on-demand data (social media),
continuous/realtime (weather data, transactions), time-series (email). Considering the four V's, the characteristic of
data frequency and size directly relates back to velocity and volume.

\noindent \textbf{Content format -} This characteristic relates back to the structure of the underlying data. Examples
of data content format include: structured (JSON, XML), unstructured (human-readable literature), semi-structured
(email).

\noindent \textbf{Data source -} This characteristic relates back to where the data originated from. As discussed
previously in~\sectref{sub:four_v}, the origin of data can have a great effect on whether or not that data is usable, as
data often varies greatly, especially when many different sources are used which may or may  not conform to a specific
content format. Another thing that is dependent on the data source is whether or not the data can be trusted.
Considering the four V's, the characteristic of data source directly relates back to veracity and variety.

\tabref{tab:ibm_data_tab}, highlights the different classes of data put forward by Mysore, et al.,
in~\cite{ibm_big_2013}. The table organises each class, along with giving a brief explanation of the class. Furthermore,
each class is related back to the previously explained characteristics in an attempt to show the connections between
class and underlying characteristics.

The classes and characteristics of data presented by Mysore et al., in~\cite{ibm_big_2013}, are highly oriented towards
industry and business users. While this is not an issue as such, as noted earlier in this section, these characteristics
and data classes may not be as relevant or appropriate for usage in other non-business domains, or even business domains
with a different focus on data.

\import{includes/tables/}{ibm_data_classes}

% subsubsection data_classification (end)


\subsubsection{Big data classification in BI\&A} % (fold)
\label{ssub:big_data_BIA}

Another major contribution to big data classification is a paper from Chen, Chiang, and Storey, focusing on the impact
of big data in the field of business intelligence and analytics~\cite{chen2012business}. Similarly to the paper looked
at in~\sectref{ssub:data_charact}, there is an emphasis on data classes and how they relate to the area of business and
organisations. However, this paper has more of an explicit focus on business, being in published in the area of business
intelligence and analytics (BI\&A). BI\&A in itself is a highly data driven field, where data is gathered and analysed
to help make informed business decisions~\cite{watson2009tutorial}.

In the paper, Chen et al.~\cite{chen2012business}, discuss the evolution of the field of BI\&A, while elaborating on
each identified stage of the evolution through highlighting of the major BI\&A applications present. For each of the
BI\&A applications presented, they attempt to show the classes of data which are deemed important, and subsequently the
characteristics associated which each class.

In Table~\ref{tab:bia_data_tab}, the classes and characteristics of data, given by Chen et
al.~\cite{chen2012business}, are presented. They are presented in terms of the BI\&A application of which they
are categorised under.

As can be seen from the data classes and characteristics identified by Chen et al.~\cite{chen2012business}, there is a
far greater variation to those previously presented by Mysore et al., in~\cite{bifet_mining_2013}. As explained earlier,
this is mainly because of the more domain specific content of this piece of literature, while the paper from Mysore et
al.~\cite{bifet_mining_2013}, while still having underlying tones of business and industry, had a less explicit focus on
their particular domain.

\import{includes/tables/}{big_data_classes}

% subsubsection characteristics_of_data_from_chen_et_al_ (end)


\subsubsection{Big data classification in contemporary organisations} % (fold)
\label{ssub:big_data_contemp}

Coming away from the business point-of-view, G\'eczy~\cite{geczy_big_2014} attempts to characterise big data in a more
generic way. He uses what he labels as ``aspects'' to determine what he believes to be the deciding characteristics of
data, in terms of the way they should be processed and also simply their intrinsic traits.

\noindent G\'eczy uses the following aspects to determine the different intrinsic characteristics of data:

\noindent \textbf{Sensitivity -} Relates to whether or not given data contains sensitive information, \ie{}personally
identifiable information, confidential information, etc. The sensitivity of the data determines the requirements relating
to how it should be handled. Often it is either a legal requirement, or in the owners' interest, to keep protected the
handled data deemed sensitive.

\noindent \textbf{Diversity -} Relates to the range of different data elements present within the data. The example given
explains the ability of smart phones to produce highly diverse data; \eg{}audio, video, location data, gyroscopic data,
etc. Having high diversity in data can both be beneficial and detrimental; diversity can add factors of complexity,
although also makes for a more rich dataset. Note that this data characteristic relates directly back to the
\emph{Variety} dimension, of the four V's.

\noindent \textbf{Quality -} Quality characteristics of data are defined to be features that affect data quality;
\eg{}completeness, accuracy, timeliness. Often the quality of data may be subject to the qualitative metrics of an
organisation, or predefined standards. The quality of data relates back to the \emph{Veracity} dimension, of the four
V's.

\noindent \textbf{Volume -} Volume refers to the size of data in terms of its basic forms of measurement, bits and bytes.
Volume is an important characteristic to take into consideration when it comes to determining the type of processing
needed. Volume, as the name suggests, directly relates back to the \emph{Volume} dimension of the four V's.

\noindent \textbf{Speed -} Data speed refers to the inflow and outflow speeds; inflow being the data that is being
acquired, while outflow being the data leaving the system (often results of computations). Different classes of data
often require different data speeds. \eg{}audio is often streamed at a far lesser speed than video, due to the
relatively low amount of data in audio when compared with video.

\noindent \textbf{Structure -} Structure relates to whether data are in structured or unstructured formats. Generally
unstructured data is more suitable for human consumption, such as literature or music. Structured data is usually
structured in such a way that it is easily able to be parsed by an algorithm, often automated by computers. The
structure of data directly relates to the difficulty of processing that data, as unstructured data usually will need
some pre-processing or artificial intelligence to process.

G\'eczy later goes on to talk about the aspects of data that relate to data processing, similar to what will be talked
about later in~\sectref{sec:big_data_processing_background}.

Overall, G\'eczy looks at data characteristics, not from any particular perspective, but from one that attempts to capture
the interests and be relevant to a number of disciplines.

% subsubsection characteristics_of_data_from_ge_czy (end)

% subsection data_classification (end)

\subsection{Discussion and analysis} % (fold)
\label{sub:classification_conclusion}

From the literature presented previously in this section, there are a number of notable points. Firstly, they were all
highly varied in the classifications and characteristics of data given. As previously stated, this can be attributed
to the variety in sources for this literature; they were all produced in the context of quite different domains, aiming
the content for most relevance in those domains. However, while there is much disconnect between the identified classes and
characteristics of data presented in the papers, there are also some similarities which show the connections
between data in different research domains.

For example, Biometrics Data, as shown in~\tabref{tab:ibm_data_tab} by Mysore et al.\cite{ibm_big_2013}, can be seen as
being related to that of data under the BI\&A application of Smart Health \& Wellbeing, as presented
in~\tabref{tab:bia_data_tab} from Chen et al.\cite{chen2012business}. By ``related'', what is meant is that the
characteristics of data underlying these classes will be similar. For example, data relating to a hospital patient's
vital signs may fall under both of these identified classes. These relations between data classes extend to G\'eczy's
classifications as well, with certain data that would fall under the class of Transaction Data, from Mysore et
al.\cite{ibm_big_2013}, also falling under the classification of being highly sensitive data, as put forward by
G\'eczy~\cite{geczy_big_2014}.

Given G\'eczy's contribution of big data classification in terms of characteristics intrinsic to
data~\cite{geczy_big_2014}, shown in~\sectref{ssub:big_data_contemp}, a lot of these characteristics can be related back
to the big data classifications given by Mysore et al.\cite{ibm_big_2013} and Chen et al.\cite{chen2012business}
in~\tabref{tab:ibm_data_tab} and~\tabref{tab:bia_data_tab}, respectively. This has been attempted to be shown
in~\tabref{tab:data_class_compare}, where the classes of big data from Mysore et al., and Chen et al., are shown in
relation to each of G\'eczy's characteristics, and whether or not they show high or low levels of those characteristics.

Note that certain data classes fall under both high and low variations of a given characteristic, and sometimes are
not present under either. This is mainly due to displaying high or low levels of a characteristic under different
contexts, or there being inconclusive information relating to the data classes' characteristics.

Also note that the data classes of machine and human generated data, as put forward by Mysore et al.\cite{ibm_big_2013},
have been emitted. In the classification given by Mysore et al., the classes of machine and
human generated data are presented as two discrete classes of data. The distinction between the two is very clear,
however it could very much be argued that these two classes should be considered super classes of which other presented
data classes could be classified under, rather than being presented as equal classes. The examples given for types of
data that fall under the classes of human generated and
machine generated data are also very vague in the way that they are presented, and could very easily fall under other
identified classes as well.

\import{includes/tables/}{data_classes_compared}

% subsection conclusion (end)

% section big_data_types_background (end)

\newpage

\section{Big data processing background} % (fold)
\label{sec:big_data_processing_background}

Much more work has been done in the area of data processing than the area related to classification of data; both in the
areas of big data and traditional data processing. Unlike data classification, which was more aimed at the classifying
of data in general, when looking at data processing, we are more interested in the relatively newer technologies which
enable the processing of big data, both in batch mode and realtime. Note that in this review, we will refer to the
processing of data in realtime as simply ``realtime data processing''. This term should be assumed to encompass the
meaning that is also often represented as ``data stream processing'', ``realtime stream processing'', and ``stream
processing''.

\subsection{Batch data processing} % (fold)
\label{sub:batch_data_processing}

Over the last decade, the main ``go-to'' solution for any sort of processing needed on datasets falling under the
umbrella of big data has been the MapReduce programming model on top of some sort of scalable distributed storage
system~\cite{bifet_mining_2013}. From a very simplified functionality standpoint, the MapReduce programming model essentially
combines the common \textbf{Map} and \textbf{Reduce} functions (among others), found in the standard libraries of many functional
programming languages, such as Haskell~\cite{lammel2008google} or even Java 8~\cite{su2014changing}, to apply a specified
type of processing in a highly parallelised and distributed fashion~\cite{yang2007map}.

The MapReduce data processing model specialises in batch mode processing. Batch data processing can be thought of where
data needed to be processed is first queued up in batches before processing begins. Once ready, those batches get fed
into the processing system and handled accordingly. %TODO: try get a reference

\subsubsection{MapReduce and GFS} % (fold)
\label{ssub:mapreduce_and_gfs}

Dean and Ghemawat, in~\cite{dean_mapreduce:_2008}, originally presented MapReduce as a technology that had been
developed internally at Google, Inc.\ to be an abstraction to simplify the various computations that engineers were
trying to perform on their large datasets. The implementations of these computations, while not complicated functions
themselves, were obscured by the fact of having to manually parallelise the computations, distribute the data, and
handle faults all in an effective manner. The MapReduce model then enabled these computations to be expressed in a
simple, high-level manner without the programmer needing to worry about optimising for available resources. Furthermore,
the MapReduce abstraction provided high scalability to differently sized clusters.

As previously stated, the MapReduce programming model is generally used on top of some sort of distributed storage
system. In the previous case at Google, Inc., in the original MapReduce implementation, it was implemented on top of
their own proprietary distributed file system, known as Google File System (GFS). Ghemawat et al.,
in~\cite{ghemawat_google_2003}, define GFS to be a ``scalable distributed file system for large distributed data-intensive
applications'', noting that can be run on ``inexpensive commodity hardware''. Note that GFS was designed and in-use
at Google, Inc.\ years before they managed to develop their MapReduce abstraction, and the original paper on MapReduce
from Dean and Ghemawat state that GFS was used to manage data and store data from MapReduce~\cite{dean_mapreduce:_2008}.
Furthermore, McKusick and Quinlan, in~\cite{mckusick2009gfs}, state that, as of 2009, the majority of Google's data
relating to their many web-oriented applications rely on GFS.

% subsubsection mapreduce_and_gfs (end)


\subsubsection{Hadoop MapReduce and HDFS} % (fold)
\label{ssub:hadoop_mapreduce_and_hdfs}

While MapReduce paired with GFS proved to be very successful solution for big data processing at Google, Inc., and
there was notable research published on the technology, it was proprietary in-house software unique to Google, and
availability elsewhere was often not an option~\cite{grossman2009varieties}. Hence, the open-source software community
responded in turn with their own implementation of MapReduce and a distributed file system analogous to GFS, known as the
Hadoop Distributed File System (HDFS). Both of these projects, along with others to date, make up the Apache Hadoop
big data ecosystem~\footnote{https://hadoop.apache.org}. The Apache Hadoop ecosystem, being a top level Apache Software
Foundation open source project, has been developed by a number of joint contributors from organisations and institutions
such as Yahoo!, Inc., Intel, IBM, UC Berkeley, among others~\cite{hadoop_committers}.

While Hadoop's MapReduce implementation very much was designed to be a functional replacements for Google's MapReduce,
HDFS is an entirely separate project in its own right. In the original paper from Yahoo!~\cite{shvachko2010hadoop},
Inc., Shvachko et al.\ present HDFS as ``the file system component of Hadoop'' with the intention of being similar to
the UNIX file system, however they also state that ``faithfulness to standards was sacrificed in favour of improved
performance''.

While HDFS was designed with replicating GFS' functionality in mind, several low-level architectural and design decisions
were made that substantially differ to those documented in GFS. For example, in~\cite{borthakur2007hadoop}, Borthakur
documents the method HDFS uses when it comes to file deletion. Borthakur talks about how when a file is deleted in HDFS,
it essentially gets moved to a \texttt{/trash} directory, much like what happens in a lot of modern operating systems.
This \texttt{/trash} directory is then purged after a configurable amount of time, the default of which being six hours.
To contrast with this, GFS is documented to have more primitive way of managing deleted files. Ghemawat, et al.,
in~\cite{ghemawat_google_2003}, document GFS' garbage collection implementation. Instead of having a centralised
\texttt{/trash} storage, deleted files get renamed to a hidden name. The GFS master then, during a regularly scheduled
scan, will delete any of these hidden files that have remained deleted for a configurable amount of time, the default
being three days. This is by far not the only difference between the two file systems, this is simply an example of a
less low-level technical difference.

% subsubsection hadoop_mapreduce_and_hdfs (end)


\subsubsection{Pig and Hive} % (fold)
\label{ssub:pig_and_hive}

Given the popularity of Hadoop, there were several early attempts at building further abstractions on top of the
MapReduce model, which were met with a high level of success. As highlighted earlier, MapReduce was originally designed
to be a nice abstraction on top of the underlying hardware, however according to Thusoo et al., in~\cite{thusoo2009hive},
MapReduce was still too low level resulting in programmers writing programs that are ``are hard to maintain and reuse''.
Thus, Thusoo et al.\ built the Hive abstraction on top of MapReduce. Hive allows programmers to write queries in a
similarly declarative language to SQL --- known affectionately as \emph{HiveQL} --- which then get compiled down into
MapReduce jobs to run on Hadoop~\cite{thusoo2010hive}.

Another common abstraction that was developed prior to Hive was what is known simply as Pig. Like Hive, Pig attempts to
be a further higher level abstraction on top of MapReduce, which ultimately compiles down into MapReduce jobs, although
what differentiates it from Hive is that instead of being a solely declarative SQL-like language, it is more of a mix of
procedural programming languages while allowing for SQL-like constraints to be specified on the data set to define the
result~\cite{olston2008pig}. Olston et al.\ describe Pig's language --- known as \emph{Pig Latin} --- to be what they
define as a ``dataflow language'', rather than a strictly procedural or declarative language.

Furthermore, note that Pig and Hive, being high level abstractions on top of MapReduce, also enable many of their own
optimisations to be applied to the underlying MapReduce jobs during the compilation
stage~\cite{gates2009building,thusoo2010hive} as well as having the benefit of being susceptible to manual query
optimisations, familiar to programmers familiar with query optimisations from SQL~\cite{gruenheid2011query}.

% subsubsection pig_and_hive (end)

% subsection batch_data_processing (end)


\subsection{Realtime data processing} % (fold)
\label{sub:realtime_data_processing}

With HDFS being an open source project with a large range of users~\cite{hadoop_users} and code
contributors~\cite{hadoop_committers}, it has grown as a project in the last few years for uses beyond what it was
originally intended for; a backend storage system for Hadoop MapReduce. HDFS is now not only used with Hadoop's
MapReduce but also with a variety of other technologies, a lot of which run as a part of the Hadoop ecosystem.
Big data processing has moved on from the more ``traditional'' method of processing, involving MapReduce jobs, which
were most suitable for batch processing of data, to those methods which specialise in the realtime processing of data.
The main difference of which is that rather than waiting for all the data before processing can be started, in realtime
data processing the data can be streamed into the processing system in realtime at any time in the whole process.

Comparing batched data processing to realtime data processing, it is useful to relate back to the four V's
identified in~\sectref{sub:four_v}. Velocity of data is often inconsistent with realtime processing, while in batch mode
processing, where you are processing the data that has already arrived and is waiting in batches to be processed, the
velocity can be considered consistent. Veracity of data is often not expected to be as consistent in realtime, as
sometimes there might be times where data does not arrive or only certain parts of the data arrive at certain times.
A realtime processing system, often called a data stream processing system (DSPS) in other literature, needs to be able
to deal with these timeliness issues, while a batch data processing system may expect everything that needs to be there
to be available.

\subsubsection{Hadoop YARN} % (fold)
\label{ssub:apache_hadoop_yarn_}

As previously looked at, the focus of the MapReduce model was performing distributed and highly parallel computations
on distributed batches of data. This suited a lot of the big data audience, and hence Hadoop became the dominant
method of big data processing~\cite{liu_survey_2014}. However for some more specialised applications, such as
the realtime monitoring of sensors, stock trading, and realtime web traffic analytics, the high latency between the data
arriving and actual results being generated from the computations was not satisfactory~\cite{kamburugamuve_survey_2014}.

A recent (2013) industry survey on European company use of big data technology by Bange, Grosser, and Janoschek, noted
in~\cite{industry_bd_survey}, shows that over 70\% of responders show a need for realtime processing. In that time,
there has certainly been a response from the open-source software community, responding with extensions to more
traditional batch systems, such as Hadoop, along with complete standalone DSPS solutions.

On the Hadoop front, the limitations of the MapReduce model were recognised, and a large effort was made in developing
the ``next generation'' of Hadoop so that it could be extensible and used with other programming models, not locked into
the rigidity of MapReduce. This became known officially known as YARN (Yet Another Resource Negotiator). According to
the original developers of YARN, Vavilapalli et al.\ state that YARN enables Hadoop to become more modular, decoupling
the resource management functionality of Hadoop from the programming model (traditionally, MapReduce)~\cite{vavilapalli2013apache}.
This decoupling essentially allowed for non-MapReduce technologies to be built on top of Hadoop, still interacting with the
overall ecosystem, allowing for much more flexible applications of big data processing on top of the existing robust
framework Hadoop provides.

Examples of such systems now built, or in some cases ported, to run on top of Hadoop, providing alternative processing
applications and use cases include:

\begin{itemize}
  \item Dryad, a general-purpose distributed execution system from Microsoft Research~\cite{isard2007dryad}. Dryad is
  aimed at being high level enough to make it ``easy'' for developers to write highly distributed and parallel applications.
  \item Spark, a data processing system, from researchers at UC Berkeley, that focuses on computations that reuse the same working data set over multiple
  parallel operations~\cite{zaharia2010spark}. Spark, and in particular Spark Streaming, will be looked at further in~\sectref{ssub:spark_streaming}.
  \item Storm, a realtime stream processing system~\cite[p.\ 244]{murthy2013apache}. Performs specified processing on an
  incoming stream of data indefinitely, until stopped. Storm will be looked at further in~\sectref{ssub:storm}.
  \item Tez, an extensible framework which allows for the building of batch and interactive Hadoop applications~\cite{web_tez}.
  \item REEF, a YARN-based runtime environment framework~\cite{chun2013reef}. REEF is essentially a further abstraction
  on top of YARN, with the intention of making a unified big data application server.
  \item Samza, a relatively new realtime data processing framework from LinkedIn. Discussed further in~\sectref{ssub:samza}.
\end{itemize}

These are just some of the more popular examples of applications built to interact with the Hadoop ecosystem via YARN.

% subsubsection apache_hadoop_yarn_ (end)


\subsubsection{Storm} % (fold)
\label{ssub:storm}

One very notable DSPS technology developed independently of Hadoop, and that is gaining immense popularity and growth in its
user base, is the Storm project. Storm was originally developed by a team of engineers lead by Nathan Marz at
BackType~\cite{web_storm}. BackType has since been acquired by Twitter, Inc.\ where development has
continued. Toshniwal et al.~\cite{toshniwal2014storm}\ describe Storm, in the context of its use at Twitter, as ``a
realtime distributed stream data processing engine'' that ``powers the real-time stream data management tasks that are
crucial to provide Twitter services''~\cite[p.\ 147]{toshniwal2014storm}. Since the project's inception, Storm has seen mass
adoption in industry, including among some of the biggest names, such as Twitter, Yahoo!, Alibaba, and
Baidu~\cite{storm_users}.

While Storm does not run on top of YARN, there is currently a large effort from engineers at Yahoo!, Inc.\ being put
into a YARN port for Storm, named ``storm-yarn''~\cite{web_storm_yarn}. This YARN port will
allow applications written for Storm to take advantage of the resources managed in a Hadoop cluster by YARN. While still
in early stages of development, ``storm-yarn'' has begun to gain attention in the developer community, through focus from
channels such as the Yahoo Developer Network~\cite{web_yahoo_blog} and Hortonworks~\cite{web_hortonworks_blog}.

% subsubsection storm (end)


\subsubsection{Spark Streaming} % (fold)
\label{ssub:spark_streaming}

Spark is another popular big data distributed processing framework, offering of both realtime data processing
and more traditional batch mode processing, running on top of YARN~\cite{zaharia2010spark}. Spark was developed at UC
Berkeley, and is notable for its novel approach to in-memory computation, through Spark's main data abstraction which is
termed a \emph{resilient distributed dataset} (RDD). An RDD is a set of data on which computations will be performed,
which can be specified to be cached in the memory across multiple machines. What this then allows is multiple distributed
operations being performed on this same dataset in parallel. A further benefit from the design of Spark is the reduce of
overhead from IO operations. Spark is designed with highly iterative computations in-mind, where the intermediate data
at each iteration stays in memory without being written and read to the underlying storage system (\eg{}HDFS).

As stated earlier, Spark allows the processing of data in realtime and batch mode. Originally, Spark was released as a
project that simply focused on batch processing, however after the need for realtime processing became apparent, an
extension project, Spark Streaming, was initiated. Spark Streaming uses a different programming model that involves what
is labelled as ``D-Streams'' (discretised streams), which essentially lets a series of deterministic batch computations
be treated as a realtime data stream~\cite{zaharia2012discretized}. The D-Stream model is specific to the Spark Streaming
system --- the original batch mode Spark system continues to use the previously mentioned RDD abstraction --- and the
creators claim performance improvements of being $2-5\times$ faster than  other realtime data processing systems, such as S4
and Storm~\cite{zaharia2013discretized}. However, this has since been disputed~\cite{web_slideshare_b}.

Both Spark and Spark Streaming have started to gain notable usage in both industry and research projects in academia in
the last few years. Online video distribution company, Conviva Inc., report to be using Spark for the processing of
analytics reports, such as viewer geographical distribution reports~\cite{web_spark_conviva,zaharia2012fast}. The Mobile
Millennium project at UC Berkeley~\cite{web_spark_mmp}, a traffic monitoring system that uses GPS through users'
cellular phones for traffic monitoring in the San Francisco Bay Area, has been using Spark for scaling the main
algorithm in use for the project: an expectation maximisation (EM) algorithm that has been parallelised by being run on
Spark~\cite{hunter2011scaling}.

% subsubsection spark_streaming (end)

\subsubsection{Samza} % (fold)
\label{ssub:samza}

Samza is a relatively new realtime big data processing framework originally developed at LinkedIn, which has since been
open-sourced at the Apache Software Foundation~\cite{web_samza}. Samza offers much similar functionality to that of
Storm, however instead the running of Samza is highly coupled with the Kafka message broker, which handles the input
and output of data streams. Essentially, Kafka is a highly distributed messaging system that focusses on the handling
of log data~\cite{kreps2011kafka}, integrating with the Hadoop ecosystem.

While Samza is lacking in maturity and adoption rates, as compared to projects such as Storm, it is built on mature
components, such as YARN and Kafka, and thus a lot of crucial features are offloaded onto these platforms. For example,
the archiving of data, stream persistence, and imperfection handling is offloaded to Kafka~\cite{bockermann2014survey}.
Likewise, YARN is used for ensuring fault tolerance through the handling of restarting machines that have failed in a
cluster~\cite{bockermann2014survey}.

% subsubsection samza (end)

\subsubsection{S4} % (fold)
\label{ssub:s4}

S4 (Simple Scalable Streaming System) is another realtime big data processing framework that originated at Yahoo!, Inc.\
that has since been open-sourced~\cite{neumeyer2010s4}. It is a relatively old project compared to the before-mentioned projects,
with development becoming less of a priority in the last few years. S4 was highly influenced by the MapReduce programming
model that was discussed in~\sectref{ssub:mapreduce_and_gfs}.

Much like what was said about Samza in~\sectref{ssub:samza}, S4 attempts offload several lower level tasks to more
mature and established systems specialising in those areas. The logical architecture of S4 lays out its jobs in a
network of processing elements (PEs) which are arranged as a directed acyclic graph. Each of these PEs entail the type
of processing to be done on the data at that point in the network. Each of the PEs are assigned to a processing node, a
logical host in the cluster. The management and coordination of these processing nodes is offloaded by S4 to
ZooKeeper~\cite{kamburugamuve_survey_2014}. Much like the before-mentioned Kafka, ZooKeeper in itself is its own complex
service used as a part of many different big data infrastructures, including Samza. ZooKeeper specialises in the high-performance
coordination of distributed processes inside distributed applications~\cite{hunt2010zookeeper}.

% subsubsection s4 (end)

% subsection realtime_data_processing (end)

\subsection{Discussion and analysis} % (fold)
\label{sub:processing_conclusion}

From the previously covered literature, it is rather difficult to provide a reasonable comparison for all the different
realtime data processing projects. A lot of the claims made in original literature relating to the projects cannot be
quantified fairly, as comparisons or tests have not been carried out relating to other projects. Instead, Stonebraker,
\c{C}\~entintemel, and Zdonik proposed what they claim to be the 8 requirements for realtime data processing systems~\cite{stonebraker_8_2005},
which can be used to given an impartial comparison of the previously covered projects. The requirements were defined a
number of years prior to the creation of the four main realtime data processing systems that were covered (2005), however are highly
cited as being the defining features that the current generation of realtime data processing systems have strived to meet.
The requirements put forward by Stonebraker et al.\ are summarised as follows:

\noindent \textbf{1: Keep the data moving -} This requirement relates to the high mobility of data and importance of low
latency in the overall processing. Hence, processing should happen as data moves, rather than storing it first, then
processing what is stored.

\noindent \textbf{2: SQL on streams -} This requirement states that a high-level SQL-like query language should be
available for performing on-the-fly queries on data streams. SQL is given as an example, however it is noted the language's
operators should be more oriented to data streams.

\noindent \textbf{3: Handle stream imperfections -} Given the high degree of imperfections in data streams, including
factors such as missing and out-of-order data, this requirement states that processing systems need to be able to handle
these issues. Simply waiting for all data to arrive if some is missing is not acceptable.

\noindent \textbf{4: Generate predictable outcomes -} This requirement relates to the determinism associated with the
outcomes of specified processes to be applied to data. A realtime processing system should have predictable and repeatable
outcomes. Note that this requirement is rather hard to satisfy as in practice data streams are, by character, rather
unpredictable. However, the operations performed on given data are required to be predictable.

\noindent \textbf{5: Integrate stored and streamed data -} This requirement states that a realtime processing system
should provide the capabilities to be able to process both data that is already stored and data that is being delivered
in realtime. This should happen seamlessly and provide the same programming interface for either source of data.

\noindent \textbf{6: Guarantee data safety and availability -} This requirement states that realtime processing systems
should ensure that they have a high level of availability for processing data, and in any cases of failures, the integrity
of data should remain consistent.

\noindent \textbf{7: Partition and scale applications automatically -} This requirement states that the partitioning of
and processing of data should be performed transparently and automatically over the hardware on which it is running on.
It should also scale to more different levels of hardware without user intervention.

\noindent \textbf{8: Process and respond instantaneously -} This requirement relates to delivering highly responsive
feedback to end-users, even for high-volume applications.

The previously covered literature has been used to determine whether or not the before-mentioned realtime data processing
systems adhere to these requirements. The outcome of this is shown in~\tabref{tab:processing_systems_compare}.

Note that in~\tabref{tab:processing_systems_compare}, those cells with ``N/A'' as the value simply mean that the literature
is inconclusive on whether or not they adhere to the particular requirement. Further investigation is required.

\import{includes/tables/}{data_processing_compare}

% subsection conclusion (end)

% section big_data_processing_background (end)


\section{Conclusion} % (fold)
\label{sec:conclusion}

The choosing of appropriate realtime data processing frameworks for the processing of a given application and dataset
is an important, and often confusing, problem. Most frameworks compare themselves in terms of performance with other
frameworks, often which are disputed by members in other framework ``camps''~\cite{web_slideshare_b,web_slideshare_a}.
Hence, providing an informed recommendation based on processing requirements and the class into which the dataset
falls is a much needed and important contribution to address the gap shown to exist in current systems.

As has been noted early in~\sectref{sub:data_classification}, there is a significant lack of research in terms of the
classification of data, and thus there is no standard metric that can be used to easily classify different data. We have
presented a comparison of existing data classification methods in~\tabref{tab:data_class_compare} that we will work with
to provide our own taxonomy of data which can be used for the appropriate classification of data under several domains.
The main sources, presented in~\sectref{sub:data_classification}, have been chosen as the basis for the development of
the data taxonomy to address this gap in existing research.

An additional point to note about the presented data classification literature, which may have been obvious but is still
of much importance, is that there was no processing recommendations given by the literature when defining each of the
different classes of data. Hence, our recommendations which will be produced will be based off our developed taxonomy of
realtime processing frameworks.

This taxonomy of realtime data processing technologies will be based off the literature relating to the realtime data
processing frameworks, as presented in~\sectref{sub:realtime_data_processing}. This taxonomy will be used, along with the
created taxonomy of data classes, to develop the previously stated processing recommendations.

This research will further the field of realtime big data processing in addressing the shown gaps, and making the
decision process far more streamlined for researchers and developers alike.

% section conclusion (end)

